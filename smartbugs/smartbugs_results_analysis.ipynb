{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "def analyze_csv_data_per_tool(df_input):\n",
    "    \"\"\"\n",
    "    Parses CSV data from a pandas DataFrame, filters data, determines ground truth\n",
    "    and predictions for Reentrancy-related findings, handles multiple runs, and\n",
    "    calculates performance metrics per toolid using sklearn.\n",
    "\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): Input DataFrame from CSV.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are toolids and values are dictionaries\n",
    "              containing calculated metrics (accuracy, precision, recall, f1_score)\n",
    "              and counts (TP, FP, TN, FN) for that tool.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()  # Work on a copy\n",
    "\n",
    "    # --- Global Preprocessing on df (before splitting by tool) ---\n",
    "\n",
    "    # 2. Determine ground truth ('actual_label') and filter ambiguous filenames\n",
    "    def determine_ground_truth(filename_val):\n",
    "        if isinstance(filename_val, str):\n",
    "            filename_lower = filename_val.lower()\n",
    "            if \"ree\" in filename_lower:  # Assumes \"ree\" in filename indicates a vulnerable sample\n",
    "                return 1\n",
    "            if \"safe\" in filename_lower:  # Assumes \"safe\" in filename indicates a non-vulnerable sample\n",
    "                return 0\n",
    "        return pd.NA  # For filenames not matching 'ree' or 'safe', or if not a string\n",
    "\n",
    "    df['actual_label'] = df['filename'].apply(determine_ground_truth)\n",
    "\n",
    "    original_row_count_before_gt_filter = len(df)\n",
    "    df = df.dropna(subset=['actual_label'])  # Remove rows with pd.NA actual_label\n",
    "    print(\n",
    "        f\"Filtered out {original_row_count_before_gt_filter - len(df)} rows with ambiguous filenames (not containing 'ree' or 'safe').\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: DataFrame is empty after filtering for 'ree'/'safe' filenames. No data to process.\")\n",
    "        return {}\n",
    "    df['actual_label'] = df['actual_label'].astype(int)\n",
    "\n",
    "    required_columns_for_core_logic = ['filename', 'findings', 'toolid', 'actual_label']\n",
    "    for col in required_columns_for_core_logic:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Critical Error: Column '{col}' is missing after initial processing. Cannot proceed.\")\n",
    "            return {}\n",
    "\n",
    "    results_per_tool = {}\n",
    "    if 'toolid' not in df.columns:\n",
    "        print(\"Critical Error: 'toolid' column is missing. Cannot group results.\")\n",
    "        return {}\n",
    "\n",
    "    unique_toolids = df['toolid'].unique()\n",
    "\n",
    "    for tool_id in unique_toolids:\n",
    "        tool_df_initial = df[df['toolid'] == tool_id].copy()\n",
    "\n",
    "        if tool_df_initial.empty:\n",
    "            continue\n",
    "\n",
    "        tool_df_deduplicated = pd.DataFrame()\n",
    "        if 'start' in tool_df_initial.columns:\n",
    "            tool_df_initial['start_numeric'] = pd.to_numeric(tool_df_initial['start'], errors='coerce')\n",
    "            valid_start_times_df = tool_df_initial.dropna(subset=['start_numeric'])\n",
    "\n",
    "            if not valid_start_times_df.empty:\n",
    "                tool_df_sorted = valid_start_times_df.sort_values(\n",
    "                    by=['filename', 'start_numeric'], ascending=[True, False]\n",
    "                )\n",
    "                tool_df_deduplicated = tool_df_sorted.drop_duplicates(subset=['filename'], keep='first')\n",
    "                print(\n",
    "                    f\"For toolid '{tool_id}', processed {len(tool_df_initial)} rows initially, kept {len(tool_df_deduplicated)} after deduplicating by filename (latest run with valid start time).\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: For toolid '{tool_id}', no valid numeric 'start' times. Using all {len(tool_df_initial)} rows, which might include duplicates.\")\n",
    "                tool_df_deduplicated = tool_df_initial\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: 'start' column not found for toolid '{tool_id}'. Using all {len(tool_df_initial)} rows, which might include duplicates.\")\n",
    "            tool_df_deduplicated = tool_df_initial\n",
    "\n",
    "        if tool_df_deduplicated.empty:\n",
    "            print(f\"Warning: No data remaining for toolid '{tool_id}' after deduplication. Skipping.\")\n",
    "            results_per_tool[tool_id] = {\n",
    "                \"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0,\n",
    "                \"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1_score\": 0.0,\n",
    "                \"error\": \"No data after deduplication\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        tool_df = tool_df_deduplicated\n",
    "        y_true = tool_df['actual_label'].tolist()\n",
    "\n",
    "        # Define keywords for reentrancy-related findings (case-insensitive).\n",
    "        # This list is crucial and MUST be updated based on how YOUR tools report reentrancy.\n",
    "        reentrancy_keywords = [\n",
    "            \"reentrancy\",\n",
    "            # Broadly catches most reentrancy mentions including specific types like _eth, _no_eth, _events, _benign, _unlimited_gas\n",
    "            \"re-entrancy\",  # Variation\n",
    "            \"reentrant\",  # Adjective form\n",
    "            \"swc_107\",  # Standard Reentrancy SWC ID (will catch SWC-107 too due to .lower())\n",
    "            \"swc-107\",  # Explicitly include hyphenated version for clarity\n",
    "            \"state_access_after_external_call\",\n",
    "            # Common pattern from Mythril indicating reentrancy (often with SWC_107)\n",
    "            \"delegatecall\",\n",
    "            # Catches findings like \"Delegatecall_to_user_supplied_address...\" and \"controlled_delegatecall\"\n",
    "            \"swc_112\",  # SWC ID for Delegatecall issues, often a reentrancy vector\n",
    "            \"swc-112\",  # Hyphenated version\n",
    "            \"re_entrancy_vulnerability\",  # Oyente\n",
    "            \"dao\"  # Securify\n",
    "        ]\n",
    "        tool_df.loc[:, 'predicted_label'] = tool_df['findings'].apply(\n",
    "            lambda x: 1 if isinstance(x, str) and (\"reentrancy_benign\" not in x.lower()) and (\n",
    "                any(keyword in x.lower() for keyword in reentrancy_keywords)) else 0\n",
    "        )\n",
    "        y_pred = tool_df['predicted_label'].tolist()\n",
    "\n",
    "        if not y_true:\n",
    "            print(f\"Warning: No valid labels collected for toolid '{tool_id}'. Skipping metrics.\")\n",
    "            results_per_tool[tool_id] = {\n",
    "                \"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0,\n",
    "                \"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1_score\": 0.0,\n",
    "                \"error\": \"No valid labels after processing\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        except ValueError as e:\n",
    "            print(\n",
    "                f\"Warning: Could not compute confusion matrix directly for toolid '{tool_id}': {e}. Manually calculating.\")\n",
    "            tp = sum((yt == 1 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "            fp = sum((yt == 0 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "            tn = sum((yt == 0 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "            fn = sum((yt == 1 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0, labels=[0, 1], pos_label=1)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0, labels=[0, 1], pos_label=1)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0, labels=[0, 1], pos_label=1)\n",
    "\n",
    "        results_per_tool[tool_id] = {\n",
    "            \"tp\": int(tp), \"fp\": int(fp), \"tn\": int(tn), \"fn\": int(fn),\n",
    "            \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1\n",
    "        }\n",
    "    return results_per_tool\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = \"results/trs.csv\"\n",
    "    df_main = pd.read_csv(csv_file_path)\n",
    "    print(f\"Successfully loaded '{csv_file_path}', shape: {df_main.shape}\")\n",
    "\n",
    "    if not df_main.empty:\n",
    "        print(\"\\nAnalyzing CSV data for Reentrancy-related findings per tool (using pandas & sklearn):\\n\")\n",
    "        all_results = analyze_csv_data_per_tool(df_main)\n",
    "\n",
    "        if not all_results:\n",
    "            print(\"No results were generated. Please check the input data and logs.\")\n",
    "        else:\n",
    "            for tool_id, metrics in all_results.items():\n",
    "                print(f\"\\nResults for toolid: {tool_id}\")\n",
    "                if \"error\" in metrics:\n",
    "                    print(f\"  Message: {metrics['error']}\")\n",
    "                elif all(k in metrics for k in [\"tp\", \"fp\", \"tn\", \"fn\"]):  # Check if full metrics dict\n",
    "                    print(f\"  True Positives (TP):  {metrics['tp']}\")\n",
    "                    print(f\"  False Positives (FP): {metrics['fp']}\")\n",
    "                    print(f\"  True Negatives (TN):  {metrics['tn']}\")\n",
    "                    print(f\"  False Negatives (FN): {metrics['fn']}\")\n",
    "                    print(\"  ------------------------------------\")\n",
    "                    print(f\"  Accuracy:             {metrics['accuracy']:.4f}\")\n",
    "                    print(f\"  Precision:            {metrics['precision']:.4f}\")\n",
    "                    print(f\"  Recall (Sensitivity): {metrics['recall']:.4f}\")\n",
    "                    print(f\"  F1-Score:             {metrics['f1_score']:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  Metrics data incomplete: {metrics}\")\n",
    "                print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\n",
    "            \"Could not proceed with analysis as DataFrame is empty (either file not found/error or dummy data creation failed).\")\n",
    "\n"
   ],
   "id": "5f8d8b5465db6518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "39452166ae87d909"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
